---
title: "I LUV COMPUTATIONAL GRAPHS"
category: "ENGINEERING"
date: "2025.09.08"
excerpt: "FLEXIBLE SCAFFOLDS FOR MODELING THE WORLD."
---

import YouTubeEmbed from '../../components/YouTubeEmbed';

Neural networks are the unromantic name for something deeply, wildly useful. At their heart they are computational graphs — little diagrams that say how numbers flow, get squashed, get mixed, and finally turn into predictions or actions. That simple idea is quietly revolutionary because it is horizontal. It works on images and on genomes, on physics simulations and on financial time series, on speech and on chemical structures. Once you accept that many problems are really about finding a function that maps inputs to outputs, computational graphs give you a programmable, differentiable toolkit to search for those functions automatically.

Below I will tell a tidy story: a quick history, an intuitive primer with the math you need, why the universal approximation idea matters, and a short detour into Demis Hassabis’ way of thinking about AI as a pattern hunter for nature itself. 

---

## A short history in one breath

The idea of modeling intelligence with networks of simple units goes back a long way. The 1940s saw McCulloch and Pitts propose a mathematical neuron. The perceptron, developed in the 1950s by Frank Rosenblatt, brought the first learning rule and public excitement. Progress slowed after criticisms in the 1960s that single-layer models could not solve problems like XOR. The second act began when researchers rediscovered and popularized backpropagation in the 1980s, which let multi-layer networks learn by propagating error gradients backward through a computational graph. The deep renaissance arrived in earnest when GPUs and large datasets made training big convolutional networks practical, capped by AlexNet’s breakthrough on ImageNet in 2012 that helped kick off the modern deep learning boom.

---

## What a neural network is, simply

Imagine you have an input vector $x$ — it might be pixel intensities, a snapshot of a molecule, or a vector describing the current state of a market. A neural network composes simple functions in a graph. One canonical building block is a fully connected layer:

$$
z = W x + b,\qquad a = \sigma(z).
$$

Here $W$ is a matrix of weights, $b$ is a bias vector, and $\sigma$ is a nonlinear activation function such as ReLU or sigmoid. The output $a$ becomes the input to the next block. A deep network stacks these blocks so the overall function is a composition

$$
f(x; \theta) = f_L(\dots f_2(f_1(x))) .
$$

Writing the model as a computational graph makes the dependencies explicit. Each node computes a small differentiable operation. That differentiability is the magic key. It allows us to compute gradients of a loss function $L(f(x; \theta), y)$ with respect to every parameter $\theta$ using the chain rule, which is the algorithmic core of backpropagation.

<YouTubeEmbed id="aircAruvnKk" title="But what is a Neural Network? - 3Blue1Brown" />

---

## How they learn, in three steps

1. Pick a loss $L$, for example mean squared error for regression or cross entropy for classification.
2. Compute the forward pass through the computational graph to get predictions.
3. Compute gradients $\nabla_\theta L$ by backpropagating error through the graph, then update parameters $\theta \leftarrow \theta - \eta \nabla_\theta L$ with a gradient-based optimizer such as stochastic gradient descent or Adam.

This is gradient descent on a high dimensional, nonconvex landscape. In practice the landscapes are weird but often forgiving: networks trained with simple recipes generalize surprisingly well.

---

## Why this is a horizontal enabler

A computational graph is domain agnostic. The same architecture concept can be adapted to images with convolutions, to sequences with recurrence or attention, to graphs with message passing, and to continuous control with policy networks. The heavy lifting is the same: choose an inductive bias appropriate to the structure of your data, encode it into the graph, and optimize. Because this pattern repeats across disciplines, neural nets have become a universal toolbox engineers and scientists can adapt instead of inventing a new algorithm from scratch for each domain.

This is not hand-waving. The mathematical underpinnings show why neural networks have immense representational power. The universal approximation theorems tell us that reasonably simple network architectures can approximate a wide class of functions arbitrarily well, given enough capacity. That gives a formal reason to believe neural networks can represent the “laws” you need to discover in data.

---

## Demis Hassabis and the idea that AI finds patterns in nature

Demis Hassabis, leader of DeepMind, often frames progress in AI as giving us tools that can discover patterns and models in the world that humans may not see directly. The intuition is this: if natural phenomena are generated by regularities, then powerful function approximators and world models built from data can uncover those regularities and turn them into predictive or generative models. In several talks he has argued that building general world models is exactly what will let AI accelerate scientific discovery, because once you can model the underlying structure you can simulate, reason, and search for interventions. That perspective sits naturally with the universality idea: neural networks are flexible pattern approximators, and if the world is structured, networks can learn that structure and exploit it.

<YouTubeEmbed id="AU6HuhrC65k" title="Dr Demis Hassabis: Using AI to Accelerate Scientific Discovery" />

---

## A quick caution

Universality is an existence theorem not a recipe. Saying a network can approximate any function does not tell you how many parameters you need, how to train it, or how to guarantee the learned model captures causal mechanisms rather than spurious correlations. For science we often want models that are interpretable and that respect invariances and symmetries. Computational graphs are flexible enough to encode these constraints explicitly, but doing so requires care and domain knowledge.

---

## Why I love computational graphs

They give us a lingua franca for learning. They turn modeling into engineering and optimization. They are compositional: small, differentiable modules can be connected to build larger, more capable systems. They are practical: you can prototype a hypothesis as a graph and let data and gradients test that hypothesis. And they scale: better compute, more data, and smarter architectures unlock qualitatively new capabilities. For anyone who likes building, experimenting, and discovering structure, computational graphs are a powerful hammer that fits many nails.

---

Computational graphs are not a single answer to everything. They are a clean, programmable language for learning. If you are curious about math, try writing down a tiny graph, compute gradients by hand, and watch how learning moves the parameters. If you are curious about science, look for the symmetries in your domain and think how they might be built into a graph. The most exciting part is this: the same simple idea scales from a classroom notebook example to models that predict protein folding, translate languages, or play superhuman games. That is why I say, plainly and without irony,

I &lt;3 computational graphs.
